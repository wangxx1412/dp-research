Here is the updated architecture and code.
To accommodate the change where the request body is strictly a list of services, the RESTful best practice is to move the tenantId to the URL Path (/v1/tenant/{tenantId}).
To handle the COMPLETED and FAILED statuses, we will introduce the Airflow Result Kafka Consumer. Because our batch grouped the requests by service before sending them to Airflow, the returning message from Airflow should indicate which requestId (the batch) and which service has finished.
1. Updated DTOs (Request & Result)
package com.example.deletion.dto;

import io.swagger.v3.oas.annotations.media.Schema;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

// 1. Updated Ingress Request DTO (Body now only contains services)
@Data
@Schema(description = "Request payload containing the list of services to delete for a specific tenant")
public class TenantDeletionRequest {
    
    @Schema(description = "List of services to delete", example = "[\"s3\", \"rds\"]")
    private List<String> services;
}

// 2. Kafka Message DTO (Sent to Airflow in the 5-min batch)
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class TenantDeletionMessage {
    private String command; // "tenant_delete"
    private String requestId; // The batch UUID
    private List<ServicePayload> payload;

    @Data
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ServicePayload {
        private String service;
        private List<String> tenantIds;
    }
}

// 3. NEW: Airflow Result DTO (Received from Airflow after processing)
@Data
@NoArgsConstructor
@AllArgsConstructor
@Schema(description = "The result payload returned by Airflow via Kafka")
public class AirflowResultMessage {
    
    @Schema(description = "The batch requestId we originally sent", example = "batch-uuid-123")
    private String requestId;
    
    @Schema(description = "The service that finished processing", example = "s3")
    private String service;
    
    @Schema(description = "COMPLETED or FAILED", example = "COMPLETED")
    private String status;
    
    @Schema(description = "Error details if status is FAILED")
    private String errorMsg;
}

2. Updated REST Controller (Swagger)
The tenantId is now a @PathVariable.
package com.example.deletion.controller;

import com.example.deletion.dto.TenantDeletionRequest;
import com.example.deletion.service.TenantDeletionService;
import io.swagger.v3.oas.annotations.Operation;
import io.swagger.v3.oas.annotations.Parameter;
import io.swagger.v3.oas.annotations.responses.ApiResponse;
import io.swagger.v3.oas.annotations.responses.ApiResponses;
import io.swagger.v3.oas.annotations.tags.Tag;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.Map;
import java.util.UUID;

@RestController
@RequestMapping("/v1/tenant")
@RequiredArgsConstructor
@Slf4j
@Tag(name = "Tenant Deletion API", description = "APIs for managing tenant data deletion")
public class TenantDeletionController {

    private final TenantDeletionService deletionService;

    @DeleteMapping("/{tenantId}")
    @Operation(
        summary = "Trigger tenant services deletion", 
        description = "Accepts a list of services to delete for a specific tenant. Requests are batched every 5 minutes."
    )
    @ApiResponses(value = {
        @ApiResponse(responseCode = "202", description = "Request accepted and queued for batch processing"),
        @ApiResponse(responseCode = "400", description = "Invalid request payload")
    })
    public ResponseEntity<Map<String, String>> triggerTenantDeletion(
            
            @Parameter(description = "The ID of the tenant", required = true, example = "tenant-12345")
            @PathVariable String tenantId,
            
            @RequestBody TenantDeletionRequest requestBody,
            
            @Parameter(description = "Original Trace ID from API Gateway", required = false)
            @RequestHeader(value = "X-B3-TraceId", required = false) String traceId,
            
            @Parameter(description = "ID of the user/admin initiating the request", required = true)
            @RequestHeader(value = "X-User-Id", required = true) String initiatorId) {

        if (traceId == null || traceId.isBlank()) {
            traceId = UUID.randomUUID().toString();
        }

        // Delegate to service to save the request to the DB
        deletionService.saveRequestsForBatching(tenantId, requestBody.getServices(), traceId, initiatorId);

        log.info("Queued deletion request for tenant: {}. TraceId: {}", tenantId, traceId);

        return ResponseEntity.accepted()
                .body(Map.of(
                    "status", "ACCEPTED",
                    "traceId", traceId,
                    "message", "Deletion request queued. It will be batched and processed within 5 minutes."
                ));
    }
}

3. Updated Repository
We need a custom method to update records in bulk when the Airflow result arrives.
package com.example.deletion.domain;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import java.util.List;

@Repository
public interface ServiceDeletionRepository extends JpaRepository<ServiceDeletionEntity, Long> {
    
    // For the 5-minute cron job to find pending requests
    List<ServiceDeletionEntity> findByStatus(String status);

    // NEW: Bulk update status when Airflow returns the result
    @Modifying
    @Query("UPDATE ServiceDeletionEntity e SET e.status = :status WHERE e.batchRequestId = :batchRequestId AND e.serviceName = :serviceName")
    int updateStatusByBatchRequestIdAndServiceName(
            @Param("status") String status, 
            @Param("batchRequestId") String batchRequestId, 
            @Param("serviceName") String serviceName
    );
}

4. Database Save Logic (Updated Service Layer)
Adapted to accept tenantId and services separately.
// Inside TenantDeletionService.java

    @Transactional
    public void saveRequestsForBatching(String tenantId, List<String> services, String traceId, String initiatorId) {
        for (String service : services) {
            ServiceDeletionEntity entity = new ServiceDeletionEntity();
            entity.setTenantId(tenantId);
            entity.setServiceName(service);
            entity.setOriginalTraceId(traceId);
            entity.setInitiator(initiatorId);
            entity.setStatus("PENDING_BATCH"); 
            entity.setCreatedAt(Instant.now());
            repository.save(entity);
        }
    }

(Note: The 5-minute @Scheduled cron job logic remains exactly the same as the previous iteration, grouping by service and generating the batchRequestId.)
5. NEW: Airflow Result Consumer
This consumer listens to the return topic, extracts the requestId, and updates the database to either COMPLETED or FAILED.
package com.example.deletion.consumer;

import com.example.deletion.domain.ServiceDeletionRepository;
import com.example.deletion.dto.AirflowResultMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

@Component
@Slf4j
@RequiredArgsConstructor
public class AirflowResultConsumer {

    private final ServiceDeletionRepository repository;

    @KafkaListener(topics = "airflow.tenant.deletion.result", groupId = "deletion-orchestrator-group")
    @Transactional
    public void processAirflowResult(ConsumerRecord<String, AirflowResultMessage> record) {
        AirflowResultMessage result = record.value();
        
        log.info("Received Airflow result. RequestId: {}, Service: {}, Status: {}", 
                 result.getRequestId(), result.getService(), result.getStatus());

        // Ensure the status is valid
        if (!"COMPLETED".equals(result.getStatus()) && !"FAILED".equals(result.getStatus())) {
            log.warn("Unknown status received from Airflow: {}", result.getStatus());
            return;
        }

        // Execute bulk update in the database for all tenants in this batch for this specific service
        int updatedCount = repository.updateStatusByBatchRequestIdAndServiceName(
                result.getStatus(),
                result.getRequestId(),
                result.getService()
        );

        if (updatedCount == 0) {
            log.warn("No records found to update for RequestId: {} and Service: {}. " +
                     "This could indicate an orphaned message or a database sync issue.", 
                     result.getRequestId(), result.getService());
        } else {
            log.info("Successfully updated {} records to {} for RequestId: {} and Service: {}", 
                     updatedCount, result.getStatus(), result.getRequestId(), result.getService());
        }

        // Handle failure specifics (e.g., trigger PagerDuty or send to DLQ)
        if ("FAILED".equals(result.getStatus())) {
            log.error("Batch Deletion FAILED. RequestId: {}, Service: {}, Error: {}", 
                      result.getRequestId(), result.getService(), result.getErrorMsg());
            // Add alert triggering logic here
        }
    }
}

