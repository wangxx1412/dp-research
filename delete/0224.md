This is a significant architectural shift. Moving from Real-time Processing to Micro-batching (Buffering) introduces a new challenge: State Management.
If we simply hold the requests in the Java application's memory (e.g., using a List) and wait for 5 minutes, a server crash or pod restart will cause data loss. Therefore, as a Senior SDE, the best practice is to persist the incoming requests to the Database immediately, and use a Cron Job (Scheduler) to sweep the database every 5 minutes, aggregate the data, and send it to Kafka.
Here is the complete, production-ready code with OpenAPI 3 (Swagger) specifications, the updated message payload, and the 5-minute batching logic.
All comments have been translated to English.
1. Data Transfer Objects (DTOs) with Swagger Annotations
We updated the payload structure to group by service instead of tenant, and replaced traceId with requestId.
package com.example.deletion.dto;

import io.swagger.v3.oas.annotations.media.Schema;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

// 1. Ingress Request DTO (Received from external services via REST)
@Data
@Schema(description = "Request payload for deleting tenant services")
public class TenantDeletionRequest {
    
    @Schema(description = "The ID of the tenant to be deleted", example = "tenant-12345")
    private String tenantId;
    
    @Schema(description = "List of services to delete for this tenant", example = "[\"s3\", \"rds\"]")
    private List<String> services;
}

// 2. Kafka Message DTO (Sent to Airflow/Stream Processor)
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class TenantDeletionMessage {
    
    @Schema(description = "The command for the stream processor", example = "tenant_delete")
    private String command; 
    
    @Schema(description = "Unique identifier for this batch request (formerly traceId)", example = "req-batch-uuid-999")
    private String requestId;
    
    @Schema(description = "Payload grouped by service containing multiple tenantIds")
    private List<ServicePayload> payload;

    @Data
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ServicePayload {
        @Schema(description = "The name of the service to clean up", example = "s3")
        private String service;
        
        @Schema(description = "List of tenant IDs whose data needs to be deleted in this service")
        private List<String> tenantIds;
    }
}

2. REST Controller with Swagger (OpenAPI 3)
The controller is now extremely fast. It only saves the request to the database and returns 202 Accepted immediately. It does not send the Kafka message.
package com.example.deletion.controller;

import com.example.deletion.dto.TenantDeletionRequest;
import com.example.deletion.service.TenantDeletionService;
import io.swagger.v3.oas.annotations.Operation;
import io.swagger.v3.oas.annotations.Parameter;
import io.swagger.v3.oas.annotations.responses.ApiResponse;
import io.swagger.v3.oas.annotations.responses.ApiResponses;
import io.swagger.v3.oas.annotations.tags.Tag;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;
import java.util.UUID;

@RestController
@RequestMapping("/v1/tenant")
@RequiredArgsConstructor
@Slf4j
@Tag(name = "Tenant Deletion API", description = "APIs for managing tenant lifecycle and data deletion")
public class TenantDeletionController {

    private final TenantDeletionService deletionService;

    @DeleteMapping
    @Operation(
        summary = "Trigger tenant services deletion", 
        description = "Accepts a deletion request and queues it. Requests are batched every 5 minutes before processing."
    )
    @ApiResponses(value = {
        @ApiResponse(responseCode = "202", description = "Request accepted and queued for batch processing"),
        @ApiResponse(responseCode = "400", description = "Invalid request payload"),
        @ApiResponse(responseCode = "401", description = "Unauthorized access")
    })
    public ResponseEntity<Map<String, String>> triggerTenantDeletion(
            @RequestBody List<TenantDeletionRequest> payloads,
            
            @Parameter(description = "Original Trace ID from API Gateway", required = false)
            @RequestHeader(value = "X-B3-TraceId", required = false) String traceId,
            
            @Parameter(description = "ID of the user/admin initiating the request", required = true)
            @RequestHeader(value = "X-User-Id", required = true) String initiatorId) {

        // 1. Generate a traceId if the upstream gateway didn't provide one
        if (traceId == null || traceId.isBlank()) {
            traceId = UUID.randomUUID().toString();
        }

        // 2. Delegate to service to merely SAVE the request to the DB (Status: PENDING)
        deletionService.saveRequestsForBatching(payloads, traceId, initiatorId);

        log.info("Accepted and queued tenant deletion request. TraceId: {}", traceId);

        // 3. Return 202 Accepted immediately (Non-blocking)
        return ResponseEntity.accepted()
                .body(Map.of(
                    "status", "ACCEPTED",
                    "traceId", traceId,
                    "message", "Deletion request queued. It will be batched and processed within 5 minutes."
                ));
    }
}

3. Database Entity & Repository
We need a status to track which records are waiting to be batched.
package com.example.deletion.domain;

import lombok.Data;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import javax.persistence.*;
import java.time.Instant;
import java.util.List;

@Entity
@Table(name = "service_deletion_requests")
@Data
public class ServiceDeletionEntity {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id; // Auto-increment ID for local DB tracking

    @Column(nullable = false)
    private String tenantId;

    @Column(nullable = false)
    private String serviceName; 

    private String originalTraceId;
    
    // The requestId generated during the 5-min batch. Used to correlate Airflow results.
    private String batchRequestId; 
    
    // Status: PENDING_BATCH, SENT_TO_AIRFLOW, COMPLETED, FAILED
    @Column(nullable = false)
    private String status; 

    private String initiator;

    private Instant createdAt;
}

@Repository
public interface ServiceDeletionRepository extends JpaRepository<ServiceDeletionEntity, Long> {
    
    // Find all records waiting to be processed in the next batch
    List<ServiceDeletionEntity> findByStatus(String status);
}

4. The Core Logic: Batch Scheduler Service
This is where the magic happens. The @Scheduled annotation acts as the 5-minute timer. It reads the database, transforms the data into your required payload: [{service: string, tenantIds: []}] format, and fires it to Kafka.
package com.example.deletion.service;

import com.example.deletion.domain.ServiceDeletionEntity;
import com.example.deletion.domain.ServiceDeletionRepository;
import com.example.deletion.dto.TenantDeletionMessage;
import com.example.deletion.dto.TenantDeletionRequest;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.Instant;
import java.util.*;
import java.util.stream.Collectors;

@Slf4j
@Service
@RequiredArgsConstructor
public class TenantDeletionService {

    private final ServiceDeletionRepository repository;
    private final KafkaTemplate<String, Object> kafkaTemplate;
    
    private static final String AIRFLOW_TOPIC = "airflow.tenant.deletion.batch";
    private static final String STATUS_PENDING = "PENDING_BATCH";
    private static final String STATUS_SENT = "SENT_TO_AIRFLOW";

    /**
     * Called by the Controller. Only saves data to DB. Extremely fast.
     */
    @Transactional
    public void saveRequestsForBatching(List<TenantDeletionRequest> payloads, String traceId, String initiatorId) {
        for (TenantDeletionRequest request : payloads) {
            for (String service : request.getServices()) {
                ServiceDeletionEntity entity = new ServiceDeletionEntity();
                entity.setTenantId(request.getTenantId());
                entity.setServiceName(service);
                entity.setOriginalTraceId(traceId);
                entity.setInitiator(initiatorId);
                entity.setStatus(STATUS_PENDING); // Mark as waiting for batch
                entity.setCreatedAt(Instant.now());
                repository.save(entity);
            }
        }
    }

    /**
     * The 5-Minute Collector (Cron Job).
     * Runs every 300,000 milliseconds (5 minutes).
     */
    @Scheduled(fixedDelay = 300000)
    @Transactional
    public void processBatchDeletions() {
        log.info("Starting 5-minute batch collection for tenant deletions...");

        // 1. Fetch all pending requests from the database
        List<ServiceDeletionEntity> pendingRequests = repository.findByStatus(STATUS_PENDING);

        if (pendingRequests.isEmpty()) {
            log.debug("No pending deletion requests found in this window.");
            return;
        }

        // 2. Generate a unique batch RequestId (This replaces the old traceId in the message)
        String batchRequestId = "batch-" + UUID.randomUUID().toString();

        // 3. Group the requests by 'serviceName' to match the required payload structure
        // Resulting Map: { "s3" -> ["tenant1", "tenant2"], "rds" -> ["tenant1"] }
        Map<String, List<String>> groupedByService = pendingRequests.stream()
                .collect(Collectors.groupingBy(
                        ServiceDeletionEntity::getServiceName,
                        Collectors.mapping(ServiceDeletionEntity::getTenantId, Collectors.toList())
                ));

        // 4. Construct the new Payload structure
        List<TenantDeletionMessage.ServicePayload> payloadList = new ArrayList<>();
        
        for (Map.Entry<String, List<String>> entry : groupedByService.entrySet()) {
            
            // Deduplicate tenantIds (just in case the same tenant sent multiple requests)
            List<String> uniqueTenantIds = entry.getValue().stream().distinct().collect(Collectors.toList());
            
            payloadList.add(new TenantDeletionMessage.ServicePayload(entry.getKey(), uniqueTenantIds));
        }

        // 5. Construct the final Kafka Message
        TenantDeletionMessage kafkaMessage = TenantDeletionMessage.builder()
                .command("tenant_delete")
                .requestId(batchRequestId) // Attach the new batch ID
                .payload(payloadList)
                .build();

        // 6. Send the batched message to Kafka
        kafkaTemplate.send(AIRFLOW_TOPIC, batchRequestId, kafkaMessage)
                .whenComplete((result, ex) -> {
                    if (ex != null) {
                        log.error("Failed to send batch message to Kafka for RequestId: {}", batchRequestId, ex);
                    } else {
                        log.info("Successfully sent batch message to Kafka. RequestId: {}, Total Services: {}", 
                                batchRequestId, payloadList.size());
                    }
                });

        // 7. Update the database records to mark them as SENT
        // We also attach the batchRequestId to the DB records so we can find them later 
        // when Airflow returns the result containing this requestId.
        for (ServiceDeletionEntity entity : pendingRequests) {
            entity.setStatus(STATUS_SENT);
            entity.setBatchRequestId(batchRequestId);
        }
        
        repository.saveAll(pendingRequests);
        
        log.info("Finished batch processing. Updated {} records to SENT_TO_AIRFLOW.", pendingRequests.size());
    }
}

Important Setup Note for Spring Boot
To make the @Scheduled annotation work, you must add @EnableScheduling to your main Spring Boot application class:
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.scheduling.annotation.EnableScheduling;

@SpringBootApplication
@EnableScheduling // <-- REQUIRED FOR THE 5-MINUTE BATCH TO RUN
public class DeletionApplication {
    public static void main(String[] args) {
        SpringApplication.run(DeletionApplication.class, args);
    }
}

