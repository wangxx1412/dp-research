这是一个非常标准且优秀的 Spring Boot 工程实践。引入 Service 层（Interface + Implementations）不仅能让职责分离 (Separation of Concerns)，还能方便我们在测试环境和生产环境之间切换策略，或者通过配置文件动态决定使用同步还是异步流程。

这里我将整个流程重新梳理，抽象出 `TenantDeletionService` 接口，并提供 `Sync` 和 `Async` 两个实现类。**所有的代码注释都已经翻译为英文。**

### 1. Data Transfer Objects (DTOs)

```java
package com.example.deletion.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import java.util.List;
import java.util.Map;

// 1. Ingress DTO (Received from REST API)
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class TenantDeletionMessage {
    private String traceId;
    private String command; // Expected: "tenant_delete"
    private List<TenantPayload> payload;
    private String initiator; // Tracks who triggered it (e.g., "admin-uuid")

    @Data
    @NoArgsConstructor
    @AllArgsConstructor
    public static class TenantPayload {
        private String tenantId;
        private List<String> services; // e.g., ["s3", "rds", "email"]
    }
}

// 2. Airflow Trigger DTO (Sent to Airflow Kafka Topic)
@Data
@Builder
public class AirflowTriggerMessage {
    private String jobId;     // Primary key in our DB, used for correlation
    private String traceId;   // Distributed tracing ID
    private String dagId;     // The specific Airflow DAG to execute
    private Map<String, Object> conf; // Parameters passed to Airflow
}

// 3. Airflow Result DTO (Received from Airflow)
@Data
public class AirflowResultMessage {
    private String jobId;     // Must be returned by Airflow to identify the request
    private String status;    // "SUCCESS" or "FAILED"
    private String errorMsg;
}

```

### 2. Entity & Repository (Database Layer)

```java
package com.example.deletion.domain;

import lombok.Data;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;
import javax.persistence.Column;
import javax.persistence.Entity;
import javax.persistence.Id;
import javax.persistence.Table;
import java.time.Instant;

@Entity
@Table(name = "service_deletion_requests")
@Data
public class ServiceDeletionEntity {
    
    @Id
    private String jobId; // UUID, sent to Airflow as correlation ID

    @Column(nullable = false)
    private String tenantId;

    @Column(nullable = false)
    private String serviceName; // e.g., "s3", "rds"

    private String traceId;
    
    private String status; // PENDING, SENT_TO_AIRFLOW, COMPLETED, FAILED

    private String initiator;

    private Instant createdAt;
    private Instant updatedAt;
}

@Repository
public interface ServiceDeletionRepository extends JpaRepository<ServiceDeletionEntity, String> {
    
    // Used by the finalizer to check if all services for a tenant are processed
    long countByTenantIdAndStatusNot(String tenantId, String status);
}

```

### 3. Service Interface

```java
package com.example.deletion.service;

import com.example.deletion.dto.TenantDeletionMessage;
import java.util.List;

public interface TenantDeletionService {
    
    /**
     * Processes the tenant deletion request.
     * * @param payloads    List of tenants and their services to be deleted.
     * @param traceId     Distributed tracing ID.
     * @param initiatorId The user or admin who initiated the request.
     */
    void processDeletionRequest(List<TenantDeletionMessage.TenantPayload> payloads, String traceId, String initiatorId);
}

```

### 4. Service Implementations (Sync vs Async)

#### A. Async Implementation (Event-Driven)

This implementation simply drops the message into the ingress Kafka topic and immediately returns. The actual DB writing and Airflow triggering happen in the background consumer.

```java
package com.example.deletion.service.impl;

import com.example.deletion.dto.TenantDeletionMessage;
import com.example.deletion.service.TenantDeletionService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Slf4j
@Service("asyncTenantDeletionService")
@RequiredArgsConstructor
public class AsyncTenantDeletionServiceImpl implements TenantDeletionService {

    private final KafkaTemplate<String, Object> kafkaTemplate;
    private static final String INGRESS_TOPIC = "tenant.deletion.ingress";

    @Override
    public void processDeletionRequest(List<TenantDeletionMessage.TenantPayload> payloads, String traceId, String initiatorId) {
        
        TenantDeletionMessage message = TenantDeletionMessage.builder()
                .traceId(traceId)
                .command("tenant_delete")
                .payload(payloads)
                .initiator(initiatorId)
                .build();

        // Use the first tenantId as the Kafka partition key to ensure ordering
        String kafkaKey = payloads.get(0).getTenantId(); 
        
        // Send asynchronously to the ingress topic (non-blocking)
        kafkaTemplate.send(INGRESS_TOPIC, kafkaKey, message)
            .whenComplete((result, ex) -> {
                if (ex != null) {
                    log.error("Failed to enqueue async deletion message for TraceId: {}", traceId, ex);
                }
            });

        log.info("Async deletion request enqueued. TraceId: {}", traceId);
    }
}

```

#### B. Sync Implementation (Direct Database & Downstream Trigger)

This implementation skips the ingress Kafka topic. It writes directly to the database and sends the trigger messages to Airflow within the same HTTP request thread.

```java
package com.example.deletion.service.impl;

import com.example.deletion.domain.ServiceDeletionEntity;
import com.example.deletion.domain.ServiceDeletionRepository;
import com.example.deletion.dto.AirflowTriggerMessage;
import com.example.deletion.dto.TenantDeletionMessage;
import com.example.deletion.service.TenantDeletionService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.Instant;
import java.util.Map;
import java.util.UUID;

@Slf4j
@Service("syncTenantDeletionService")
@RequiredArgsConstructor
public class SyncTenantDeletionServiceImpl implements TenantDeletionService {

    private final ServiceDeletionRepository repository;
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private static final String AIRFLOW_TRIGGER_TOPIC = "airflow.pipeline.trigger";

    @Override
    @Transactional // Ensures DB save and Kafka send are atomic in this thread
    public void processDeletionRequest(List<TenantDeletionMessage.TenantPayload> payloads, String traceId, String initiatorId) {
        
        log.info("Processing sync deletion request for TraceId: {}", traceId);

        for (TenantDeletionMessage.TenantPayload tenantPayload : payloads) {
            String tenantId = tenantPayload.getTenantId();

            for (String serviceName : tenantPayload.getServices()) {
                
                // 1. Generate unique JobId (The correlation ID)
                String jobId = UUID.randomUUID().toString();

                // 2. Persist metadata to Database synchronously
                ServiceDeletionEntity entity = new ServiceDeletionEntity();
                entity.setJobId(jobId);
                entity.setTenantId(tenantId);
                entity.setServiceName(serviceName);
                entity.setTraceId(traceId);
                entity.setInitiator(initiatorId);
                entity.setStatus("SENT_TO_AIRFLOW");
                entity.setCreatedAt(Instant.now());
                repository.save(entity);

                // 3. Construct and send trigger message directly to Airflow Topic
                String targetDagId = serviceName + "_cleanup_dag"; 

                AirflowTriggerMessage triggerMsg = AirflowTriggerMessage.builder()
                        .jobId(jobId)
                        .traceId(traceId)
                        .dagId(targetDagId)
                        .conf(Map.of(
                            "tenantId", tenantId,
                            "serviceToClean", serviceName
                        ))
                        .build();

                kafkaTemplate.send(AIRFLOW_TRIGGER_TOPIC, tenantId, triggerMsg);
                
                log.info("Synchronously triggered Airflow DAG {} for JobId {}", targetDagId, jobId);
            }
        }
    }
}

```

### 5. Controller (API Gateway)

The Controller uses `@Qualifier` to inject the specific implementation. You can easily switch this via `@ConditionalOnProperty` in a real-world scenario if you want to control it via `application.yml`.

```java
package com.example.deletion.controller;

import com.example.deletion.dto.TenantDeletionMessage;
import com.example.deletion.service.TenantDeletionService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.http.ResponseEntity;
import org.springframework.util.StringUtils;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;
import java.util.UUID;

@RestController
@RequestMapping("/v1/tenant")
@Slf4j
public class TenantDeletionController {

    private final TenantDeletionService deletionService;

    // Injecting the async implementation by default for better throughput
    // Change "asyncTenantDeletionService" to "syncTenantDeletionService" to switch modes
    public TenantDeletionController(@Qualifier("asyncTenantDeletionService") TenantDeletionService deletionService) {
        this.deletionService = deletionService;
    }

    @DeleteMapping
    public ResponseEntity<Map<String, String>> triggerTenantDeletion(
            @RequestBody List<TenantDeletionMessage.TenantPayload> payloads,
            @RequestHeader(value = "X-B3-TraceId", required = false) String traceId,
            @RequestHeader(value = "X-User-Id", required = true) String initiatorId) {

        // Generate TraceId if not provided by upstream API Gateway
        if (!StringUtils.hasText(traceId)) {
            traceId = UUID.randomUUID().toString(); 
        }

        // Delegate business logic to the injected Service
        deletionService.processDeletionRequest(payloads, traceId, initiatorId);

        log.info("Accepted tenant deletion request. TraceId: {}", traceId);

        // Immediately return 202 Accepted
        return ResponseEntity.accepted()
                .body(Map.of(
                    "status", "ACCEPTED",
                    "traceId", traceId,
                    "message", "Deletion process initiated."
                ));
    }
}

```

### 6. Kafka Consumers

#### A. Ingress Consumer (Only active if using Async implementation)

Notice how its logic is almost identical to `SyncTenantDeletionServiceImpl`. If you strictly follow DRY principles, you could extract the inner loop into a shared helper class, but keeping it here makes the event-driven architecture easier to read.

```java
package com.example.deletion.consumer;

import com.example.deletion.domain.ServiceDeletionEntity;
import com.example.deletion.domain.ServiceDeletionRepository;
import com.example.deletion.dto.AirflowTriggerMessage;
import com.example.deletion.dto.TenantDeletionMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.time.Instant;
import java.util.Map;
import java.util.UUID;

@Component
@Slf4j
@RequiredArgsConstructor
public class TenantDeletionIngressConsumer {

    private final ServiceDeletionRepository repository;
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private static final String AIRFLOW_TRIGGER_TOPIC = "airflow.pipeline.trigger";

    @KafkaListener(topics = "tenant.deletion.ingress", groupId = "deletion-orchestrator-group")
    @Transactional 
    public void processIngressMessage(ConsumerRecord<String, TenantDeletionMessage> record) {
        TenantDeletionMessage message = record.value();
        
        if (!"tenant_delete".equals(message.getCommand())) {
            log.warn("Unknown command received in ingress topic: {}", message.getCommand());
            return;
        }

        log.info("Async consumer processing ingress deletion command for TraceId: {}", message.getTraceId());

        for (TenantDeletionMessage.TenantPayload tenantPayload : message.getPayload()) {
            String tenantId = tenantPayload.getTenantId();

            for (String serviceName : tenantPayload.getServices()) {
                
                String jobId = UUID.randomUUID().toString();

                ServiceDeletionEntity entity = new ServiceDeletionEntity();
                entity.setJobId(jobId);
                entity.setTenantId(tenantId);
                entity.setServiceName(serviceName);
                entity.setTraceId(message.getTraceId());
                entity.setInitiator(message.getInitiator());
                entity.setStatus("SENT_TO_AIRFLOW");
                entity.setCreatedAt(Instant.now());
                repository.save(entity);

                String targetDagId = serviceName + "_cleanup_dag"; 

                AirflowTriggerMessage triggerMsg = AirflowTriggerMessage.builder()
                        .jobId(jobId) 
                        .traceId(message.getTraceId())
                        .dagId(targetDagId)
                        .conf(Map.of("tenantId", tenantId, "serviceToClean", serviceName))
                        .build();

                kafkaTemplate.send(AIRFLOW_TRIGGER_TOPIC, tenantId, triggerMsg);
            }
        }
    }
}

```

#### B. Result Consumer (Required for both Sync and Async)

```java
package com.example.deletion.consumer;

import com.example.deletion.domain.ServiceDeletionEntity;
import com.example.deletion.domain.ServiceDeletionRepository;
import com.example.deletion.dto.AirflowResultMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.time.Instant;

@Component
@Slf4j
@RequiredArgsConstructor
public class AirflowResultConsumer {

    private final ServiceDeletionRepository repository;
    // Assume NotificationService is an existing component in your system
    // private final NotificationService notificationService; 

    @KafkaListener(topics = "airflow.pipeline.result", groupId = "deletion-orchestrator-group")
    @Transactional
    public void processAirflowResult(ConsumerRecord<String, AirflowResultMessage> record) {
        AirflowResultMessage result = record.value();
        
        // 1. Find the original request using the correlation ticket (jobId)
        ServiceDeletionEntity entity = repository.findById(result.getJobId())
            .orElseThrow(() -> new RuntimeException("Orphaned Airflow result! JobId: " + result.getJobId()));

        log.info("Received Airflow result for Tenant: {}, Service: {}, Status: {}", 
                 entity.getTenantId(), entity.getServiceName(), result.getStatus());

        // 2. Update status
        entity.setStatus(result.getStatus()); // e.g., "SUCCESS" or "FAILED"
        entity.setUpdatedAt(Instant.now());
        repository.save(entity);

        // 3. Handle failure
        if ("FAILED".equals(result.getStatus())) {
            log.error("Deletion failed for JobId {}. Error: {}", result.getJobId(), result.getErrorMsg());
            // Trigger alerts (e.g., PagerDuty) or send to Dead Letter Queue
            return;
        }

        // 4. Finalizer Pattern: Check if this was the last pending service for the tenant
        long pendingTasks = repository.countByTenantIdAndStatusNot(entity.getTenantId(), "SUCCESS");
        
        if (pendingTasks == 0) {
            log.info("All services for Tenant {} have been successfully deleted. Triggering final notification.", entity.getTenantId());
            
            // Execute final GDPR actions here:
            // 1. Send final confirmation email
            // notificationService.sendTenantDeletionComplete(entity.getTenantId(), entity.getInitiator());
            // 2. Perform Crypto-shredding (Destroying the tenant's KMS key)
        }
    }
}

```

这种设计模式结构清晰，扩展性强。是否需要我提供一份结合 Kubernetes 部署这个 Spring Boot 应用的 `deployment.yaml` 基础配置？
